%section-start setup
\documentclass{beamer}
%section-start declare document information!
\title[SMC Generation]{Sequential Monte Carlo for Constrained Generation}
\subtitle{An Application of Generative Models}
\author{Hannah Nelson}
\institute{University of California, Santa Barbara}
\date{2025}
%section-end
%section-start load packages!
\usepackage{graphicx}
\usepackage[backend=biber]{biblatex}
\usepackage{xcolor}
\usepackage{soul} % use the package named soul by Melchior Franz for highlighting
%section-end
%section-start add bibliography
\addbibresource{refs.bib}
%section-end
%section-start define macros
%section-start shiftleft
% A simple command to shift stuff left. meant to shift itemizes left because beamer doesn't play well with left margins for some reason.
\newcommand\shiftleft[1]{
    \begin{columns}
        \begin{column}{0.1\textwidth}
        \end{column}
        \begin{column}{0.9\textwidth}
            #1
        \end{column}
    \end{columns}
}
%section-end
%section-end
%section-end
%section-start document
\begin{document}
%section-start titlepage
\begin{frame}
\titlepage
\end{frame}
%section-end
%section-start overview
\begin{frame}
\frametitle{Overview}
    \begin{itemize}
        \item The Problem: Contstrained Generation
        \item What is Sequential Monte Carlo?
            \begin{itemize}
                \item Importance Resampling
                \item Repeated Importance Sampling
            \end{itemize}
        \item A Selection of Solutions
            \begin{itemize}
                \item Feynman Kac Diffusion Steering (FKDS)
                \item Sequentially Ordered SMC (SOSMC)
                \item LLM Probabilistic Programming (LLaMPPS)
            \end{itemize}
    \end{itemize}
\end{frame}
%section-end
%section-start what is the problem?
%section-start the problem
\begin{frame}
\frametitle{The Problem: Constrained Generation}
    Imagine we want to:
    \begin{itemize}
        \item Generate an image which fits a description.
        \item Fill in a blank in text.
        \item Make a tree which doesn't intersect objects.
    \end{itemize}
    We need to generate a \textbf{solution} which obeys a \textbf{constraint}
\end{frame}
%section-end
%section-start visualizations of the problem.
\begin{frame}
\frametitle{The Problem: Visualizations}
    \includegraphics[totalheight=3cm]{resources/infilling.png}
    \footnote{\textcite{lew2023sequentialmontecarlosteering}}
    \includegraphics[totalheight=3cm]{resources/tree_growth.png}
    \footnote{\textcite{10.1145/2766895}}
    \includegraphics[totalheight=3.5cm]{resources/text_to_image.png}
    \footnote{\textcite{singhal2025generalframeworkinferencetimescaling}}
\end{frame}
%section-end
%section-start the approaches
\begin{frame}{The Problem: Possible Approaches}
    \begin{columns}[t] % [t] aligns content to the top
        % --- Column 1 ---
        \begin{column}{0.3\textwidth}
            \textbf{Optimization Based}
            \begin{itemize}
                \item Beam Search
                \item Expectation Maximization
                \item Simulated Annealing
                \item Greedy Setpwise Generation
            \end{itemize}
        \end{column}
        % --- Column 2 ---
        \begin{column}{0.3\textwidth}
            \textbf{Conditional Based}
            \begin{itemize}
                \item Importance Sample
                \item Monte Carlo Markov Chain
                \item Classifier Free Guidance
                \item
                    \hl{Sequential Monte Carlo}
            \end{itemize}
        \end{column}
        % --- Column 3 ---
        \begin{column}{0.3\textwidth}
            \textbf{Hueristic Based}
            \begin{itemize}
                \item Wave Function Collapse
                \item Context Engineering
                \item Hueristic EM
                \item Chain of Thought
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}
%section-end
%section-end
%section-start what is smc?
%section-start we want smc
\begin{frame}
\frametitle{We Want SMC Because}
    SMC strikes a blance:
    \begin{itemize}
        \item High Sample Diversity
        \item Moderate Speed
        \item Reduced Variance
        \item Known Asemptotic Behavior
    \end{itemize}
\end{frame}
%section-end
%section-start importance resampling
\begin{frame}
\frametitle{A Naive Approach: Importance Resampling}
    \includegraphics[totalheight=8cm]{resources/importance_resampling.png}
\end{frame}
%section-end
%section-start smc
\begin{frame}
    \frametitle{A Key Improvement: Sequential Monte Carlo (SMC)}
    \includegraphics[totalheight=8cm]{resources/smc_as_iterated_importance_sampling.png}
\end{frame}
%section-end
%section-end
%section-start what are the examples?
%section-start fkds
\begin{frame}
\frametitle{Feynman Kac Diffusion Steering}
    \shiftleft{
        \begin{itemize}
            \item[The Paper:]
                \citetitle{singhal2025generalframeworkinferencetimescaling}
            \item[The Problem:]
                Generate an image which fits a satisfies a classification.
            \item[The Difficulty:]
                Classifier models cannot give accurate scores on partially generated images.
        \end{itemize}
    }
\end{frame}
%section-end
%section-start fkds approach
\begin{frame}
\frametitle{FKDS: Approach}
1. Identify the constraint a valid solution must solve:

    $$\prod_{t=T}^0G_t(x_T,\dots,x_t,c)=exp(\lambda r(x_0,c))$$

2. Make solutions which weigh themselves using approximate rewards obeying constraint:

    $$G_t(x_T,\dots,x_t,c)=exp(\lambda (r_\phi(x_t,c)-r_\phi(x_{t+1},c)))$$

3. Select an appropriate approximate reward.

    $$r_\phi(x_t,c) = r(\mathbb{E}[x_t|x_0],c) \text{ or trained objective}$$

    This generates empirically better label matching images than \textbf{classifier free guidance (CFG)}!
\end{frame}
%section-end
%section-start sosmc
\begin{frame}
    \frametitle{Stochastically Ordered Sequential Monte Carlo (SOSMC)}
    \shiftleft{
        \begin{itemize}
            \item[The Paper:]
                \citetitle{10.1145/2766895}
            \item[The Problem:]
                3d Generation uses a higherarchial recursive generation. SMC requires a flat generation order.
            \item[The Difficulty:]
                Naively flattening the structure gets poor results due to early steps potentially making later steps nearly impossible.
        \end{itemize}
    }
\end{frame}
%section-end
%section-start sosmc approach
\begin{frame}
\frametitle{SOSMC: Approach}
The solution taken: Randomize the order!
\newline
\newline
orderings which take early steps that make later steps impossible now get eliminated.
\newline
\newline
By randomizing the order, we execute in all orders and then get rid of the bad ones.
\newline
\newline
    SOSMC exceeds the performance of \textbf{metropolis hastings} at low compute allocations in 3d generation examples.
\end{frame}
%section-end
%section-start LLaMPPL
\begin{frame}
    \frametitle{Language Model Probabalistic Programing (LLaMPPL)}
    \shiftleft{
        \begin{itemize}
            \item[The Paper:]
                \citetitle{lew2023sequentialmontecarlosteering}
            \item[The Problem:]
                We need our models to fill in blanks in sentences.
            \item[The Difficulty:]
                Our blanks have are of unknown length. Since language models are discreet, duplicate particles will exist.
        \end{itemize}
    }
\end{frame}
%section-end
%section-start LLaMPPL approach
\begin{frame}
\frametitle{LLaMPPL: Approach}
To reduce duplicate particles:
    \begin{itemize}
        \item
    Turn the resampling into a upsampling step followed by a no replacement downsampling step.
    \end{itemize}
For variable length blanks:
    \begin{itemize}
        \item
Propose a length with a geometric distribution.
        \item
Counteract the bias with reweighting.
    \end{itemize}
LLaMPPL allows higher sample diversity than \textbf{beam search}, while having similar complexity.

\end{frame}
%section-end
%section-end
%section-start in summary
\begin{frame}
\frametitle{In Summary}
    \begin{itemize}
        \item Sequential Monte Carlo can guide generation
        \item The papers adapt it to:
            \begin{itemize}
                \item Approximate intermediate rewards
                \item Hierarchial samplers
                \item Discreet token sequences
                \item Variable length generations
            \end{itemize}
        \item It has benefits:
            \begin{itemize}
                \item Adheres to guidance well
                \item Maintains sample diversity
                \item Lower sample variance
            \end{itemize}
    \end{itemize}
\end{frame}
%section-end
%section-start Citations
\begin{frame}
\frametitle{Citations}
    \printbibliography
\end{frame}
%section-end
\end{document}
%section-end
